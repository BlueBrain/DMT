#+TITLE: DMT: Data, Models, and Tests...

* TODO write tests for physical dimensions.

* Model
  #+begin_src
  There is no correct model. But some may be useful.
  #+end_src

  What is a model anyway?
  The models we are interested in are quantitative models of some real
  phenomenon, that must be validated against reality. This project is about how
  to go about the business of validating models. However, we first need to
  decide what we mean by models, and what we do not. We can think of our models
  as simulations of reality.

** Simulation
   It is natural to think of BBP's brain model as a simulation --- in fact we
   run /simulations/ on the circuit. With that in mind, how will we define what
   a simulation is?

   Google:

   #+begin_src

   1. imitation of a situation or process.
   /eg: simulation of blood flowing through arteries and veins/
   2. the action of pretending; deception.
   /eg: clever simulation that's good enough to trick you/
   3. the production of a computer model of something, especially for the
   purpose of study.
   /eg: the method was tested by computer simulation/
   #+end_src
   All three are subjective --- they assume that a human is reading the
   definition. There are many undefineds here:
   1. What is a /situation/ or a /process/ ?
   2. What is imitation? A monkey imitating a human? Or your child imitating
      you?
   3. What is pretending? Now that is a complicated thought! How do we even we
      begin to understand what pretension is!
   4. Definition number 3 is the most useful one for our purposes. It neatly
      redirects the reader to /model/. May be /model/'s definition will be
      simpler to understand. But there will again will be a huge list of
      possible meanings --- which one should we pick?


   Now consider that there is such a /simulation/ that matches
   reality (of humans) perfectly. For a /real/ human, it will be clear which one
   is the simulation. They will be biased. Of course they have to be able to
   sense which one is their side. So with that in mind, the first definition
   makes sense. We are the originals, reality is original, simulation is an
   imitation. We have that knowledge wired in to our /physics/. We are
   inherently biased to think of our current reality as the limit of simulation
   as it gets more sophisticated.

   How would we define /simulation/ for a machine? But can you make an
   /objective/ definition that can be and avoid the bias of /reality as the
   limit of simulation/.


   Wikipedia:

   #+begin_src
   ...the imitation of the operation of a real world process or system.
   ...first requires that a model be developed; this model represents the key
   characteristics, behaviors and functions of the selected physical or abstract
   system or process. The model represents the system itself, whereas the
   simulation represents the operation of the system over time.
   #+end_src

   Wikipedia has a very interesting article on [[https://en.wikipedia.org/wiki/Simulation][Simulation]]. Don't forget to read
   it!

** Validation as an /as objective definition of simulation as possible/.
   To figure out reality from simulation, you should know that simulation is the
   imitation. So if you can tell the imitation from reality, you know which one
   is the simulation. This has been a common science-fiction theme.

   The solution is to test, imitation against reality --- under as
   similar circumstances as possible. The smaller the difference, the harder it
   is to tell simulation from reality. So, from a machine's perspective we can
   only tell if the two are the same or different. The machine may as well be
   biased that the simulation is in fact the reality because both phenomena of
   validation and simulation happen /in-silico/. So be it, it does not matter.
   All we want to find out is if the two are the same or not.

   Once we can objectively establish the extent to which a /model/ or
   /simulation/ agrees with /reality/ on a given set of phenomena, we can give a
   quantitative measure of the /amount/ of validation.

   Provided with a prior about which one is reality, the machine can then
   compute the max-apriori probability of simulation /versus/ reality.

   While that happens, we have what we really want --- and that is a
   quantitative definition of validation, even if it is implicit.


  We may not be sure about what a model is, but we do understand what we are
  talking about here. So it may not be necessary to dwell on this point for too
  long. But we will, even if for a small moment.
  Our models are scientific models that must be validated against empirical
  observations. They are /mechanical/ models, in the sense that these models are
  essentially computer simulations---computations of a set of equations that
  describe the (modeled) systems mechanics.

  What other kind of (mathematical) models are there? There are statistical
  models, and their generalization: machine learning models. However,
  statistical models are not built around differential equations.

  - Quantitative Model :: a formal computation based system that can generate
       predictions about observable quantities.
  - Scope of a Model :: the set of observable quantities that can be predicted
       by the model.
  - Validity of a Model :: accuracy of the model's predictions, as measured
       against experimental observations.


  Overall validity of a model is not a well defined, or complete, notion. A
  model, such as the reconstructed brain circuit, may be valid for a certain set
  of measurements, while very much off reality for another set. So, validation
  of a model is a continuous process, and any new set of relevant experimental
  data should trigger a new cycle of validations.

  A related, and may be even more important, goal of model validation
  against experimental data is the dissemination of experimental results in a
  consumable form. From a modeling perspective, the purpose of experiments is to
  provide data that can help improve the model. As things stand in the
  scientific world, it is fairly straightforward, if not easy, to build a model.
  There is no dearth of models, especially in lieu of the observation that all
  models are wrong. The hard part is to make models that are useful. One
  pre-condition for models to be useful is that they be validated against
  reality. This hard task of validating models is made worse by the difficulty
  of obtaining relevant data, or even complete lack of it. The relevant data
  gathered from real systems is necessary to tune model parameters, as well as
  to validate's it's predictions. In many cases, there is a lot of experimental
  data available, but not in an easy to use or easy to search format. The bar of
  using this data is too high. *A model validation framework should focus on*
  *integrating experimental results with modeling.*

* Model verification /versus/ model validation.
  The solution to a differential equation must be verified against the initial /
  boundary conditions. But that is a /verification/ and not a /validation/.

  Experimental data may be partitioned into two groups:
  1. data used to set the parameters of the model, and
  2. rest of the data.

  - Statistical models ::
       For statistical models there is only one kind of data. Some of it is used
       to train the model (/i.e./ determining model parameters), the rest for
       testing or /cross-validation/. Either way, error which is the difference
       between the model's output and the expected value (for a given input)
       determines the outcome. During training, the error determines the size
       and direction of parameter value updates. Error over the test data
       determines how good the model is.


  The /mechanistic/ scientific models we consider model the system of interest
  from the ground up. For the purpose of /validating against experimental data/
  we must *treat the model as an experimental object*.

  The data that was used to /train/ or set model parameters must then be used to
  /verify/ the model. Whatever this data was, it was measured in an experimental
  set up for a /real/ system (which is not a computer / mathematical model), or
  it may be a mock up of real data. (We may want to test by using
  a standard-deviation to randomize real experimental data). We must be able to
  measure the same type of data for a digital model of the system. We can even
  /simulate/ the experimental protocol used in the /real/ experiment. Positive
  model verification will require that the result of /in-silico/ measurement
  matches that of /in-vivo, in-vitro/ measurement.

  We may say that to verify a model is to validate it against experimental data
  that was used to set it's parameters.

  A satisfactory reproduction of the data that was used to set the model's
  parameters does not constitute the model's validation. It merely verifies that
  the model has been constructed correctly: that it's /components/ have been set
  correctly.

  According to Wikipedia [[https://en.wikipedia.org/wiki/Verification_and_validation_of_computer_simulation_models][article]]
  #+begin_src
  ...verification of a model is the process of confirming that it is correctly
  implemented with respect to a conceptual model, i. e. it matches
  specifications and assumptions deemed acceptable for the given purpose of
  application.
  ...
  Validation checks the accuracy of the model's representation of the real
  system.
  Step 1. Build a model that has high face validity.
  Step 2. Validate model assumptions.
  Step 3. Compare the model input-output transformations to corresponding
  input-output transformations for the real system.
  #+end_src

* Scientific Unit testing
  [[https://github.com/scidash/sciunit][SciUnit]] is a framework for /test-driven scientific model validation/.

  I am not sure if calling it /test-driven/ scientific model validations makes
  much sense. They are abusing the term from /test-driven development (TDD). TDD
  is a process. Wikipedia teaches us:
  #+begin_src
  ...is a software development process that relies on the repetition of a very
  short development cycle: requirements are turned into very specific test
  cases, then the software is improved to pass the new tests, only. This is
  opposed to software development that allows software to be added that is not
  proven to meet requirements.
  #+end_src
  The process of TDD:
  1. Add a test
  2. Run all tests and see if the new test fails
  3. Write the code
  4. Run tests
  5. Refactor code
  6. Repeat

  The process of TDD is based on /unit-tests/.
  - unit-test ::
       a function that checks the behavior of a single component.

  The critical word above is /single/ -- a good coding practice is to have a
  single component for a single functionality.

  In contrast to generic industrial software systems, it may not be possible to
  identify components in a model's predictions. Consider the validation of a
  brain circuit's structure. Several structural features of the modeled brain
  region's structure must be validated. These can be computed for the model --
  however these /predictions/ are not independent of each other -- and each may
  depend on more that one component of the model. One failed validation may not
  indicate where in the model the disagreement arises from.

  - regression-test ::
       ensure changes to software have not changed it's (previously defined)
       functionality.


  Clearly, validation of a scientific model has nothing to do with the process
  of it's development. So let us not confuse ourselves by trying to find
  analogies  where they do not exist.

* Scientific Validation as a Debate
  Traditional progress of science has always been led by debate. It is one man
  with his colleagues (constituting the unit of research industry: a research
  group) against another (or the whole establishment of science). The process of
  science has always included a debate. The modern format / shape that this
  debate takes is that of /peer-reviewed/ publication.

  The process of /peer-review/ can be compared to /code-review/. In fact code
  review has been called peer-review of computer source code. The goal is to
  find mistakes, and to improve the overall quality of software. Though I must
  notice that in my experience code review is an explicitly helpful exercise,
  competitive in a friendly manner. Peer review can sometimes become an outright
  shouting match.

  To /validate/ a quantitative model involves a lot of activities. In the
  current, human peer-review model, the proponents of the model must present
  their model in a /scientific/ paper. The paper must describe the model, and
  discuss how good it's predictions are, and compare those to predictions of
  other models. The validity of a model relies on the assumptions made to build
  the model, and the accuracy of it's predictions. Several of the assumptions
  can be best judged by scientists in the same field as the authors of the
  model. Many of these activities cannot be replaced by automation, others can
  be. For example, the human reviewer should not have to look for experimental
  results to compare the model against, or have to run a computation to validate
  model predictions against experimental measurements.

  The narrow focus of papers, and the print media they are published in are
  another limitation that prevents model and data integration. Both experimental
  and data papers are frozen in time. The investigators must use their knowledge
  of literature to compare their results against models or experimental results.

  Many of these requirements of model review can be fulfilled by a single
  /collaborative validation framework/.


* Validation vs Analysis
  In my previous attempts I have distinguished a circuit analysis from a
  validation. Strictly, a Validation needs to provide a judgment. An Analysis,
  on the other hand can produce just a bunch of plots. Previously, we have
  allowed such an Analysis object --- however the /analyses/ it performed was a
  comparison of two /models/. We can extend this notion of an /Analysis/ to a
  allow for analysis of a single model or data-object.

  Question to consider is where to place such a class in our /Validation/ class
  hierarchy.



* Previous work


  Of course there is SciUnitTest.

** Validation API
   Developed at BBP by Juan Pablo Palacios. None of the code referenced on the
   [[https://bbpteam.epfl.ch/project/spaces/pages/viewpage.action?spaceKey=~palacios&title=Validation+API][Confluence page]] is alive. However there are some user stories on Jira that we
   can get some ideas from.

   Requirements from the previous BBP validation framework attempts.
   1. Circuit validations be run automatically
      1. user can find documentation on how to register a circuit
      2. a set of validations are automatically executed after a user has
        registered a circuit
      3. default values for validation parameters so that validations to have as
        much automation as possible --- take away as much responsibility from
        the user as possible
      4. easy access to results, with documentation that explains where the
        results are located
   2. [[https://bbpteam.epfl.ch/project/issues/browse/LBK-457][Implement Validation Framework V1]]
      1. As a SD (scientific developer) I want to have the validations from the
        previous validation framework available as tasks
      2. As a DEV, I want a finalized spec for the Validation report object
      3. As a SU (scientific user) I want to be able to launch validation jobs
        using the python Task Framework client API with Bluepy dependencies
      4. As a SCIDEV I want the Validation report data model to include the input
        distributions in the Validation result set
      5. As an SU, I want to be able to launch jobs using scipy with the python
        Task Framework client API
      6. As a SCIDEV I want the Validation API to allow easy comparison of
        distributions stored in numpy arrays, according to acceptable validation
        practices
      7. As a SCIDEV I want a non-trivial example of a validation using the
        Validation toolkit
      8. As a DEV I want to have a standard API for producing validation reports

*** Validation Report
   There was a draft spec for validation reports. It is still  [[https://bbpteam.epfl.ch/project/spaces/display/BBPWFA/Validation+Report+-+Draft+0.2][available on
   Confluence]]. We note some observations on this report.
   This specification was about how the validation results are presented on a
   platform web interface. The /report/ is actually a summary.
   - Requirements ::
     1. Simple to understand
     2. Web-friendly format
     3. Allows multiple representations of the same data
     4. Supports various data types (e.g. 1D bar charts and histograms, 2D
        graphs)
     5. Validation input and reference distributions must be available
     6. De-coupled from graphical representation


   The goal of the older validation /toolkit/ was to produce validation reports
   with data. There are examples on the confluence page that show JSONs carrying
   data that can be plotted. However, in my experience, it is hard to assume
   what form a validation's data may be. In a framework we cannot over-specify
   data formats. We should leave the data formats, as well as how the data is
   reported, open for the user to decide. However we can decide to give the same
   treatment to data and code. For this we do have to constrain the formats that
   data must be provided in. The formats we choose must be Python friendly.
   Also, we expect data to be statistical summaries (/e.g./ mean cell
   densities), which amounts to /small/ data which we do not have to optimize by
   hand. JSON is definitely allowed, as well as any other format that can be
   loaded by Pandas (CSVs, matrices, and tables). This data will be mostly
   numbers, and the user must provide /code/ to load it. The data must be
   separate from the code, so that we can upload it to /Nexus/. We must also be
   able to update the whole /validation/ up to Nexus.


** Original Validation Framework

** My previous attempts
   I have written a /ValidationFramework/ before --- which was confused about
   being a framework for validations and toolkit to do analysis for a BBP
   circuit. We discuss features and lessons learned from writing that code.

   My previous ~ValidationFramework~ is more of a toolkit to analyze a BBP brain
   circuit, with specific well defined analyses that can be applied to circuits
   of several brain regions. Such a toolkit will definitely be useful --- but
   where can be put it? Definitely not as a part of ~dmt~. Can it go somewhere
   in ~neuro_dmt~? We can start by writing that code under
   ~neuro_dmt/bbp_circuit/~, and move any generally applicable code to a more
   general location.

  
* Plotting
  Plotting is an independent component of validation. We should strive to define
  plotting classes that follow some grammar of graphics with clean semantics.

** A grammar for plotting
   You should be able to plot the same data in different forms, with a very high
   level semantics. The grammar for plotting is a domain specific language for
   plotting. 
** Bar plots
   Bar plots are fairly easy to produce in ~matplotlib~. We already have grouped
   bars. We should review our bar plots and have it expose a clean interface
   that we can then generalize to other kinds of plots. Lets start by learning
   from ~ggplot~.
** Cross Plot
   This is a clever plot. It is applicable where we compare two data-sets, for
   example model data against a single experimental data.
   

* Practice 
** Hippocampus
   Armando presented <2018-09-05 Wed>.
   - Cell Composition Validation :: Three plots, all bar plots
     1. Whats the deal with SLM and SR in the plot? They look combined.
     2. Cell type dependent bar plots. Should these be also by layer?
     3. Interneuron fractions were plotted by their ~mtype~. Were these fractions
        computed by layer, that is was the denominator the number of cells in a
        layer, or total cells in the whole circuit?
     4. Mixed bars for overall cell density in the CA1, and PC cells in layer SP
        with bars for density by layer. This type of composite/mixed plots will
        be harder to plot. In principle we can produce any grouped bars (see
        appropriate section in Plotting).
   - Connectome Validation 1 :: Two plots probably drawn with MR's code.
     1. Bouton Density compares experimental values against model values.
     2. Synapses per connection compares experimental values against model
        values.
     These could have been bar plots, except for the large number of cell ~mtypes~.
     1. Number of efferent synapses /vs/ mtype, by cell-type. A synapse has a
        pre and post end. AR's plots are for number of synapses by /pre-mtype/,
        a different panel of bar plots by the type of post-cell. AR uses PC and
        interneuron as the type of the post-cell. We will need to add this
        category of cell types!
     2. Divergence ? Ask Armando. This can be redone to display the same
        information. *We do not understand this plot.*
        May be it is the fraction of synaptic inputs from PC vs INT to a given
        mtype. 
   - Connctome Validation 3 :: connection probability as a function of
        soma-distance.
     1. over all comparison between model and experiment at a given soma
        distance. or may be not a function of distance at all (*ASK*)
     2. mtype-->mtype ...
   - Connection Analysis 1 :: number of afferent synapses
     1. vs mtype, pre-type is PC or INT, post type is mtype...
     
* Prediction
  Validation of a model is in hindsight. Looking forward, there are experiments
  that a scientist may think of but unable to to do on the bench. However, if
  they can think of the experimental data they expect to gather, they can write
  down an analysis of that data. If the user can express as an /adapter/
  /interface/ the measurements they expect from a model, we have an /analysis/
  that can be enhanced to a /validation/ when relevant experimental data becomes
  available. Meanwhile, such a /prediction analysis/ can be compared across
  different models provided adapters for both  are available.

  ~Prediction~ can exist in our code independently from ~Analysis~ --- in fact
  as a subclass of ~Analysis~. A ~Prediction~ must have a 
  ~predicted_phenomenon :: Phenomenon~ 

  ~Prediction~ is just an ~Observation~ made by a model.
  ~Observation~ sounds like a synonym of ~Measurement~.

  #+begin_src haskell
  data Observation = Observation { phenomenon :: Phenomenon
  ~                              , measurement :: Measurement}
  #+end_src

  #+begin_src python
  class Measurement:
  ~  phenomenon = Field(
  ~     __name__ = "phenomenon",
  ~     __type__ = Phenomenon,
  ~     __doc__  = "The physical phenomenon measured"
  ~  )
  ~  units = Field(
  ~     __name__ = "units",
  ~     __type__ = Units,
  ~     __doc__  = "Physical units of this measurement"
  ~  )
  ~  quantity_type = Field(
  ~     __name__ = "quantity_type"
  ~     __type__ = type,
  ~     __doc__  = """Type of the quantity that represents the amplitude of
  ~     this measurement. A measurement is not just the outcome, but also the 
  ~     inputs --- that is each measurement consists of dependent variables,
  ~     and independent variables. 
  ~     For singleton measurements the quantity type may be a float or a 
  ~     vector depending on the phenomenon measured. For full information,
  ~     the return type must be a Pandas Series --- with a multi-level index
  ~     with a level for each of the input parameters of the measurement 
  ~     method. The benefit of returning a quantity containing the measurement
  ~     parameters is that this allows the downstream step of putting singleton
  ~     measurements into a data-frame without the need of doing anything more
  ~     than call 'pandas.concat' method on a list of pandas Series.
  ~      
  ~     For a statistical measurement we will use a pandas DataFrame containing
  ~     the mean and  standard deviation in two columns. In case the 
  ~     measurement outcome is vector valued, we may use a multi-level column
  ~     structure, akin to the multi-level index we have used to handle 
  ~     multiple measurement parameters. The code representing the measurement 
  ~     method's parameters should have the same name as used for the 
  ~     corresponding levels in the returned data-frame's index."""
  ~   ) 
  #+end_src

  If we stick to ~Pandas Series or DataFrames~, we can set requirements as 
  follows

  #+begin_src python
  class Measurement:
  ~  phenomenon = Field(
  ~     __name__ = "phenomenon",
  ~     __type__ = Phenomenon,
  ~     __doc__  = "The physical phenomenon measured"
  ~  )
  ~  units = Field(
  ~     __name__ = "units",
  ~     __type__ = Units,
  ~     __doc__  = "Physical units of this measurement"
  ~  )
  ~  parameters = Field(
  ~     __name__ = "param_names",
  ~     __type__ = list, #of the (single word) names of parameters of the measurement method
  ~     __doc__  = """Order of the list 'param_names' will correspond to"""
  ~  )
  ~  quantity = Field(
  ~     __name__ = "quantity",
  ~     __type__ = EitherType(pandas.Series, pandas.DataFrame),
  ~     __doc__  = "The magnitude of this measurement"
  ~  )
  ~    
  ~  def _check_arguments(self, params_dict):
  ~     for p in self.parameters:
  ~        if p not in params_dict:
  ~           raise ValueError(
  ~              "Required parameter {} not passed to measurement method".format(p)
  ~           )
  ~  def index(self, params_dict):
  ~     self.__check_arguments(params_dict)
  ~     n = len(self.parameters)
  ~     if n == 0:
  ~        return pandas.Index([])
  ~     if n == 1:
  ~        p = parameters[0]
  ~        return pandas.Index([params[p]], name=p)
  ~     return pandas.MultiIndex\
  ~                  .from_tuples([tuple(params[n] for n in self.parameters)],
  ~                               names=self.parameters)
  #+end_src

* Requirements
  Validation is about DMT: Data, Models, and Tests

** Introductiom

   We will attack the problem from both ends: a data, model, and test (validation)
   collaboration framework for the general quantitative scientific endeavors, and
   the other end of the use-case of validating the BBP brain region circuit.

   - General guidelines ::
        While developing, we should follow the rough guidelines of
        1. do not over-specify
        2. couple components loosely
        3. leave enough documentation, and expressive code


   We will use cell density as a guiding example.
   We know the mathematical definition of cell density. As we consider that
   definition, we can also see how open-ended it is. For a spatially extended
   system, you can measure a field like quantity, such as a cell density, in
   what ever which way you want. In the particular case of the cerebral cortex,
   you may want cell density at the center of each layer, or computed across the
   whole layer, or sampled in small boxes spread across a layer to asses the
   spatial fluctuations in the cell density. You may even decide that there is
   nothing sacrosanct about the layers --- you can decide to look at density as
   a function of the cortical depth. It is easy to imagine yourself as a real
   experimental neuroscientist and performing this experiments in the wet. Once
   again, the different ways to compute cell density at a given location remain
   the same as those in each layer.

   So there are many possible ways to compute cell density. However each one of
   them will measure cell density --- they are all methods to measure the same
   phenomenon. So we have it: *CellDensity* is a *Phenomenon*. We can define
   *Phenomenon* as a record:
   #+begin_src haskell
   data Phenomenon = Phenomenon { name :: String
                                , label :: String
                                , description :: String}
   #+end_src

   Along with (total) cell density, consider the inhibitory cell density. So,
   instead of all cells, you count only inter-neurons in your /simulated/ wet
   experiments. The phenomenon being measured now is *InhibitorCellDensity*.
   However, both *CellDensity* and *InhibitorCellDensity* can be quantified by
   the same *Quantity*. The exact form of this *Quantity* will depend on the
   measurement method, and whether measurement method was statistical. However,
   the underlying physical dimensions of the two phenomena is the same: [Count /
   Volume], count per unit volume.

   A *Measurement* is a /record/ of the *Phenomenon* (that can distinguish
   CellDensity from InhibitoryCellDensity), the *MeasurableSystem* (which will
   either be a model, or a data-object), and the *Quantity* that resulted from
   measuring the /system/.

   #+begin_src haskell
   type MagnitudeType

   data Quantity =  Quantity { unit :: Unit
                             , magnitude :: MagnitudeType }

   data Measurement = Measurement { phenomenon :: Phenomenon
                                  , system :: MeasurableSystem
                                  , quantity :: Quantity }

   #+end_src

   I have abused my incompetence in Haskell syntax. What I want to say is
   that *Quantity* is a generic type, parameterized by the type of it's
   magnitude. Normally you would assume that the type of a quantity's magnitude
   should be a floating point number. However that would severely  limit it's
   use in our work. So we have to accommodate all the different /quantities/
   that the different possible methods of measuring a cell density into a
   generic type parameter. Luckily in Python we can pretend that all of them are
   ducks, a duck is a duck so is a stack of ducks. All we have to ensure that
   the ~class Quantity~ quacks. Defining *Quantity* in C++ should also be simple
   --- after all C++ meta-programming is duck typing at static compilation time.
   Haskell should be a challenge --- but not too hard --- just gotta find out
   what the syntax is. In any case ...

   Getting back to *Phenomenon*, and the example of two /circuit composition/
   *Phenomenon*  we can also see the utility of a domain specific *Phenomenon*
   hierarchy. We are stepping into the territory of ontology, which may be worth
   it we manage to keep it simple. We can implement this hierarchy for the domain
   --- so keeping *Phenomenon* abstract may be sensible. The class hierarchy of
   the domain's *Phenomenon*s can be used to structure a class hierarchy of a
   group of validations.

   Remember that one of our aims is to leverage Python's provided libraries
   (especially Abstract Base Classes) to build features that may be useful in a
   collaborative effort to define validations against data, that may be applied
   to different model types. Keeping that in mind, here are some big questions

   1. Who should determine the method of measuring the cell density?
      The comparing (validating, judging, testing) method would call that
      method on the provided ~model~ argument --- it's absence should display a
      nice explanatory message that such and such method needs to be implemented
      by the model adapter ...
      So, the validation has to assume that a certain method is available on the
      ~model~ parameter. It is this /assumption/ that we need to express in an
      abstract base class. The writer of validation will have to write such a
      class.
   2. Can we expect our average user to be able to write Python abstract
     classes? Probably not.
     So we need to provide a /front-end/ simple Python way of writing a Python
     abstract base class. On the back-end such code should become ABC's.
     We cannot solve all problems at the same time, neither can we think and
     plan for each desired feature. So we should take the first step --- and
     write the validations ourselves with ABC's.

   3. How should we organize circuit validations, and adapter models?

** Three parties
   Our validation framework has three dimensions and hence three different types
   of user-roles. Of course the same user can play all three users.
   Nevertheless, it is important to note the ideal specification of each of
   these three user-roles.

   - Validation writer ::
        The role in which the user writes the validation. The user must follow
        the guidelines / documentation of the validation base class...
   - Model adapter ::
        The role in which the user decides to adapt their model to the
        requirements of a particular validation / set of validations.
   - Data interface author ::
        The role in which the user provides the data to the validation
        repository and programs a data-reader so that the data can be loaded in
        a validation. If the user wants their data to be used in the context of
        a particular available validation, they must provide a reader that
        satisfies the requirements of that validation. This role can be split
        into two: 1. the role in which the user uploads the data, and 2. the
        role in which the user adapts an available data to the requirements of a
        validation.

** Validation Report
   There are three levels to a validation:
   - Describe a validation ::
        The /abstract/ code, that defines a validation --- it expresses the
        scientific motivation and definitions, the mathematical and statistical
        methods, the algorithm, and the programmatic logic. The code of a
        validation must clearly describe its data / model requirements.
   - Validating interactively ::
        A validation in context of a specific pair of model and data. This
        should be mostly automated, however we can provide some options. For
        example, the user may want to interactively generate data from a model.
        In an /interactive validation/ the user should be allowed to process the
        data into a format specified in the validation's documentation. This
        feature will allow the user to interactively validate models against
        data. This should operate as a  validation of two data-sets. In fact, we
        will program validation as a validation of two data-sets that will then
        be used to validate a model against a data-set or even two models.
        Interactive validation should be particularly useful when models are
        simple and can be run in up to a few minutes ---  for example in a MOOC
        exercise.
   -  Validation as data ::
        Running a validation must result in a report that should tell us if the
        validation was successful, and explain the scientific motivation,
        statistical methodology, as well as the algorithmic details. The report
        should provide plots and resulting data in a folder and upload to
        Nexus or another data-base. We can require all the three participants in
        a validation to provide their abstracts and methods as part of their
        code.

** Metadata
   We need several kinds of meta-data.
*** Author
    Author's name, affiliation, basic / simple things.
    As a collaboration platform, we could use ~Author~ as a synonym for ~User~.

    1. Every entity that needs to be implemented by the user must have an Author.

*** Citation
    We can use ~Citation~ to store references --- we can leverage any available
    library for these purposes --- some Python classes for bibtex.
*** Caption
    It might be simpler to keep caption as a string property that a concrete
    Validation should implement. However, it may be interesting to think of what
    data forms a Caption.
** Reporting
   Reporting is very important. In fact, a Report is what a Validation is good
   for. The user must be able to generate a report for fully implemented
   validation. Reporting can get arbitrarily complex ---  Imagine a study of a
   model against a suite of experimental data. Such a study may be considered as
   a validation report!

   In fact, it seems to me that /reporting/ is the pivot around which we can
   explore the possibilities of a Data/Model/Test Validation framework.

   <2018-08-08 Wed> Until now we have thought of /Cheetah templates/ as integral
   to what we have been programming. And we have been programming primarily the
   backend. To make durable progress we will need to shift our attention to the
   front-end, and to do that we have to start thinking of the separation of the
   back-end and the front-end.


   <2018-08-08 Wed> Given that we require a front back split, we will assume
   that we will use a /controller/ to relay queries / responses back and forth,
   front to back, view to model to view. We shall not concern ourselves must
   with the front-end for now. However we will respond with JSON from the
   controller, that will receive PODs from the models.

*** Back and front ends.
    Back end is most of the work. Front end is simple, and we set the conditions
    / guidelines to separate the two.

    1. let Front End be concerned with Cheetah / Jinja2, or any other templating
       system.
    2. let Front End be concerned with html, or pdf, or text, or whatever format
       the output is desired in.
    3. let Back End concern itself with producing a Report.
    4. let Report be a POD (plain old data) --- named tuple, a data-only class.
    5. let Front End have a query system that the user can manipulate to monitor
       their validations.


    A few precautions are also necessary
    1. Some amount of entanglement between the front and back ends is
       unavoidable.
    2. Entangled design is all right to begin with, but must be noticed and
       dealt with as early as possible.


    Guidelines for double-ended development
    1. Front End will decide how to display results of a query.
    2. Front End will send a query to Back End.
    3. Back End will respond with a properly structured JSON.


    The trick is in determining the fields of this JSON. The simplest, and
    probably the most useful one, is to allow a collaboration between the two
    ends.

**** Front End queries, front end user stories
     What may the user query at the interactive web-page?
     1. A user may come in looking for available validations to test her model.
     2. A user may come in with data for measurement of some phenomenon, and
        look for models that study that phenomenon.
     3. A user may come in with data for measurement of some phenomenon, and
        look for other experimental studies of that phenomenon.
     4. A user may come in with a model wanting to compare it with other models
        that study the same phenomenon.


    One story may evolve like this.
    1. User has a new model for a phenomenon.
    2. User logs in.
    3. User uploads her model.
    4. User fills in a meta-data form for her model.
    5. User asks for available validations.
    6. User browses the available relevant validations.
    7. User reads about the validated phenomenon:
       - the /simulated-experimental/ methodology employed in sampling the
         modeled system to measure the validated phenomenon
       - the statistical methodology used for judging a model's performance
       - the experimental studies from where the data was sourced to validated
         the model's predictions
       - what the validations report discuss, to judge the quality of
         documentation
    8. User chooses a validation that she found appropriate to her requirements.
    9. User finds that she must provide an adaptor for her model to run her
       chosen validation.
    10. User looks up the documentation and follows a tutorial to write an
        adaptor.
    11. User submits her adaptor, and allows the system to criticize her code.
    12. User manages to write an acceptable adaptor with the help of the system.
    13. User submits the adaptor, signs it with her affiliation and motivation.
    14. User runs the validation with her adaptor.
    15. User reads the report produced by the validation.
    16. User decides to write her own validation, because either no validation
        was available for her use, or the ones she found were not satisfactory.
    17. User queries for available data for the phenomena that her model
        studies.
    18. User also digs / cooks up experimental data from other sources.
    19. User submits her collected experimental data.
    20. User decides to publicize submitted data, by providing proper credits,
        and provenance.
    21. User reads documentation and follows tutorials to write a validation for
        her model.
    22. User runs the validation she has written.
    23. User reads the report produced by her validation.
    24. User submits all the code and data required for her validation  or
        produced by her validation for /publication/.
    25. User decides to start a public discussion that may be mediated by chosen
       /peers/ in the system.
    26. User subscribes for notifications about DMvTs of her interest, /i.e./
        any data, models, validation tests submitted by other users for
        publication.
    27. User decides to keep data for her private use, if she cannot provide
        proper credits and provenance.
    28. User runs her validations in a private sandbox.
    29. User appoints a chosen set of /peers/ for judging the logic of her model
        and validation.
    30. User appoints a chosen set of /peers/ for code-review.

***** Validation by humans
      We can think of peer-review of academic papers as validation by humans,
      instead of an automated system. Strict guidelines may be set for such a
      validation --- probably by asking the judges to list a small number of
      criteria. Each of these criterion and its requirements can then be made as
      objective as possible, and each of them provided with a scale to measure
      the response. Once such a list has been curated, it can be presented to
      the any fresh reviewer. So there are two stages of development here. This
      human validation needs humans to compile!

      We have a ready example / application of this idea here at BBP. That of
      determining the type of an experimentally observed morphology. Suppose I
      come up with a model to construct a random sample of a morphology of a
      particular type. The /algorithm/ to produce of a model morphology can
      itself be considered as a model --- a model of the processes that produce
      a morphology. Of course, this model is not what happens in reality, there
      will be a lot of missing links, and the model may be completely abstract.
      The point is that we want to plug in this model somewhere else in the
      bigger puzzle involving the simulation of the human brain. So the human
      /validator/ will look at the model's output and experimentally
      morphologies and provide their judgment.



*** Use cases
    We need to catalog different use cases
     - Scientific reports ::  A study of a model against a suite of experimental
          data, or a suite of a models against a set of experimental dataset, or
          a suite of models against a suite of experimental datasets. Not all
          such analyses may be considered a ~Validation~, but we can definitely
          call them an ~Analysis~.
     - Jupyter Notebook ::
   In addition to scientific papers, analysis code in a jupyter notebook could be
   converted into a validation report. The simplest way to do this is to ask
   the author of a notebook to mark their classes and methods with specific
   decorators.

   Best way to find out the requirements of Jupyter Notebook to Validation is to
   repeat one well-practiced analysis manually, /i.e./ to repeat all the logic
   of a validation such as cell density.

*** Decorating
    As a start, we will implement a decorator ~@reportattribute~, like
    ~@requiredmethod~ and ~@adaptermethod~. The problem this address is the
    following:




** Aggregated Validation
   Validating a model against a single measurement is not a full validation of
   the model. In fact, a model can never be fully validated. Validation of a
   model is a continuous process. So what should we call an object that will
   compare a model against an experiment for only one measurement (such as cell
   density) ? We can call it ~TestCase~ or ~ValidationTestCase~.

   - TestCase :: A validation is composed of /tests/. We validate a model
        against reality with a battery of test cases.
        #+begin_src
        A test case is a specification of the inputs, execution conditions,
        testing procedure, and expected results that define a single test to be
        executed to achieve a particular software testing objective, such as to
        exercise a particular program path or to verify compliance wth a
        specific requirement.
        #+end_src
        We should define individual test cases, ~ValidationTestCase~.
        #+begin_case python
        class ValidationTestCase(BaseClass):
          """a 'unit' test for the model"""
          pass
        #+end_case
        What should ~ValidationTestCase~ derive from?
        Is it a ~CircuitAnalysis~?

    If we think of a validation as testing a model against reality for a single
    phenomenon, then the complete definition of a Validation must contain the
    experimental data.



** Kinds of Judges
   Or course a Judge should be as objective as possible --- a program if at all
   possible. If not, and this might be the case quite often, human eyeballs. We
   need a mechanism to allow users to judge the results of an Investigation.
   This consideration makes us rethink our design of the process. Verdict should
   be given after Investigation / Analysis, and then a final Report.
   #+begin_src
   Analysis => AnalysisReport --> Verdict --> Validation => ValidationReport
   #+end_src
   In the scheme outlined above, an Analysis should produce an ~AnalysisReport~
   that is used to produce a ~Verdict~. The ~AnalysisReport~ and ~Verdict~ are
   then combined by ~Validation~ to produce a ~ValidationReport~.

* Action items
  A list of items to do.
** TODO Build a class hierarchy of circuit *Phenomenon*.
   CREATED: <2018-06-28 Thu>
** DONE <2018-06-29 Fri> Write about the three user-roles:
   1. the user who provides wrapper for a model
   2. the user who provides the data, and a reader for that data
   3. the user who codes a validation
** TODO write about previous Validation Frameworks
   CREATED: <2018-06-29 Fri>
   SCHEDULED: <2018-06-30 Sam>
** TODO write about circuit validation vs circuit analysis.
   SCHEDULED: <2018-06-30 Sam>
** TODO write about simulation
   There is a good looking [[https://en.wikipedia.org/wiki/Verification_and_validation_of_computer_simulation_models][wikipedia article]] titled "Verification and validation
   of computer simulation models".
   SCHEDULED: <2018-06-30 Sam>
** TODO implement meta-data classes
   SCHEDULED: <2018-07-02 Mon>
   So far we have ~Author~, ~Reference~.
** TODO locate models and data-objects
   SCHEDULED: <2018-07-09 Mon>
   We want to query models and data repo to see which of these provide
   measurements of a given phenomenon.
** TODO write a class that creates Fields
   SCHEDULED: <2018-07-09 Mon>


** TODO think about an Analysis --- more general,
   SCHEDULED: <2018-07-20 Fri>
   ~class Analysis~ encompasses any study of models and data.
   or may be even a ~class Study~
** TODO imagine an interaction between the user and the interactive web-page
   SCHEDULED: <2018-08-08 Wed>
   We keep thinking about users --- some parts of our back-end have used that
   idea as motivation. However the user will see the front-end. We must think
   about what we want to offer the user --- /a well defined UI/. What
   constitutes a good definition of a UI?
** TODO define a measurement as discussed in a section above.
   SCHEDULED: <2018-09-13 Thu>
   With the current design of the code, there is no natural place or a natural
   way to define a ~Measurement~.
   The simplest way to have the same affect as a ~class Measurement~ is to 
   require same functionality inside the ~measurement.Method~ subclass. So we 
   for example, will have the following:
   #+begin_src python
   class CellDensity(measurement.Method):
   ~   """..."""

   ~   label = "in-silico"
   ~   phenomenon = Phenomenon("Cell Density", "Number of cells in a unit volume")
   ~   units = "1000/mm^3"
   ~   parameters = ["roi"]
   ~
   ~   def __init__(self, circuit):
   ~       """This intializer can be placed in the super class... even if 
   ~   helper may not always be needed."""
   ~       self.__circuit = circuit
   ~       self.__helper  = BlueBrainModelHelper(circuit=circuit)

   ~   def __call__(self, **params):
   ~      """...Call Me..."""
   ~      self._check_arguments(params)
   ~      plist = [params[p] for p in self.parameters]
   ~      value = 1.e-3 * self.__helper.get_cell_density(*plist)
   ~      return Measurement(
   ~         quantity=pd.Dataframe([{self.label: value}], index=self.index(params),
   ~         units=self.units
   ~         phenomenon=self.phenomenon
   ~         parameters=self.parameters
   ~      )
   #+end_src
   To improve the previous implementation, we should insist on using subclasses 
   of our ~class Parameter~. This way ~parameters~ will be a ~List[Parameter]~.
   This will enforce better documentation for each measurement, that can slowly
   build into a library of measurements.
   If we enforce return of pandas objects from a singleton measurement, we can
   easily stack up a bunch of these measurements and make statistical summaries.
 

* Ontology
  I will need to have my own /ontology/ of the brain to build my code against.
  This ontology is not going to allow me to file and retrieve data --- but
  provide a naming scheme that enables expression of scientific and programming
  ideas. 

** Phenomena, Parameter, and Measurements
   These concepts are inter-related.
   - Phenomenon :: A phenomenon is anything that manifests itself. A phenomenon
        may be an observable fact, or event. A phenomenon may be described by a
        system of information related to matter, energy, or space-time.
   - Parameter :: a numerical or other measurable factor forming one of a set
        that defines a system or sets the conditions of its operation. A limit
        or boundary which defines the scope of a particular process or activity.
   - Measurement :: assignment of a number to a characteristic of an object or
        event.
   - Method :: specifies /measurement/ of a /phenomenon/.

        
   So, we /measure/ a /phenomenon/. ~Phenomenon~, ~Measurement~, and ~Method~
   are okay, their conceptual connotations are already part of our code-base.
   The problem is with ~Parameter~.  Is ~Parameter~ also a ~Phenomenon~?
   However, we cannot follow that line of logic too far. Everything that can be
   assigned a number is a ~Phenomenon~!

   We can use the definition presented above only as a guideline. For us, the
   definition works if we have defined ~System~:
   
   - System :: Model, reality, experiments and measurements of these, or their
        parts, each of these can be thought as a system. A circuit is a system
        that is composed of smaller system --- cells. But cells are not the only
        micro-systems embedded inside the circuit. Synpatic processes can also
        be described as a system. To /isolate/ a system in our /understanding/
        we have to do two excercises: 
     1. Identify the processes involved /inside/ the system, /i.e./ the
        players and their roles.
     2. And, describe the system's boundary. That is specify what /is/ the
        system, and what /is not/ the system.

*** Measurement. As a system. MeasurementSystem.
    Consider description of a system again:
     1. Identify the processes involved /inside/ the system, /i.e./ the
        players and their roles.
     2. And, describe the system's boundary. That is specify what /is/ the
        system, and what /is not/ the system.


     Describing the interior is easy. The harder part is to describe the
     boundary. This is where ~Parameter~ comes. A ~Parameter~ set describes a
     system. What a ~Parameter~ set really describes is the /boundary/ of a
     system.  Consider measurement as a system.

     Consider ~Measurement~ of a ~CellDensity~, as a /function/ of ~Layer~. We
     might have defined a ~LayerCellDensity~ that would be more descriptive.
     Instead, we decided to define ~CellDensity~ as a ~measurement.Method~ ...
     /i.e./ ~CellDensity~ describes (to the computer!) a method to compute a
     cell density. /This method has to operate at as low a level as possible./
     What we mean, in the context of cell density, is that we should explicitly
     provide a measurement method for cell density as a function a of location
     in space. Computing cell density will involve computing a volume and
     counting cells inside that volume. Exact details of the process of
     computing cell density should be part of the  description of ~CellDensity~,
     and must be implemented by the model adapter. However, the model adapter
     must use a language that can be shared with users of the adapter, that is
     testers (validators), and experimentalists. We need to /set boundaries/ of
     such a communication. We can say

     #+begin_src python
       cell_density =\
           self.adapter\
               .get_cell_density(
                   circuit_model,
                   by=cortical_layer)
     #+end_src
     
     Here ~cortical_layer~ is a ~Parameter~ because it defines the scope of the
     measurement being requested in that method call.

*** Parameter
    Should we require a ~Parameter~ to describe what it quantifies? 
    The name ~cortical_layer~ is fairly clear to read, but the computer should
    be able to translate the /code/ of ~cortical_layer~ into behavior.
     
*** Observation
    M. Fowler talks considers ~Observation~ as a super-type of ~Measurement~,
    that covers categorical variables in addition to quantitative variables.
     


* TODOS
** TODO make armando's validations use bluebrain adapters
*** TODO use comparison/validation for hugo's port of armando-validations
** TODO improve overall circuit measurements
*** TODO get total number of cells and volume according to atlas
    will not be statistical
*** TODO method for total cell count of different types, synapse types
    allow region_of_interest = None in bbphelpers.get_cell_counts
    in order to extract cell counts for entire circuit
** TODO write template for AnalysisReport
*** TODO write template for ModelComparisonReport
** TODO Simplify
*** TODO use voxcell.RegionMap.find instead of hierarchy in CircuitSpecialization
*** TODO remove CircuitSpecialization entirely?
** TODO change the interaction with SpatialParameters
   i.e. hide them behind the Adapter: 
     the adapter recieves a string representative of the biological parameter
     the SpatialParameter subclass corresponding to it is defined as a part
     of the adaptor, and it maps the string to it to know what to apply.
     this way, one is not required to implement SpatialParameters for one's own
     Adapter to be usable for the validation

* Circuits
  Specifics of circuits build at BBP teach us about validations. 
* Presentations
  Talk about validations.
** <2019-03-01 Fri>
   
