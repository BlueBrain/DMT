{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dmt.model import Adapter, Interface, AIBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMT Nexus\n",
    "DMT Nexus is a service to validate your models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class EntityType(Enum):\n",
    "    DATA = \"data\"\n",
    "    ANALYSIS = \"analysis\"\n",
    "    \n",
    "class Data:\n",
    "    \"\"\"Data with some meta-data to track.\"\"\"\n",
    "    def __init__(self,\n",
    "        phenomenon,\n",
    "        data,\n",
    "        provenance):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        -----------\n",
    "        phenomenon: Phenomenon measured by this data. \n",
    "        data: a pandas data-frame containing values for this data.\n",
    "        provenance: a dictionary telling where did this data come from?\"\"\"\n",
    "        self,phenomenon = phenomenon\n",
    "        self.data = data\n",
    "        self.provenance = provenance\n",
    "        \n",
    "    \n",
    "    \n",
    "class DMTNexus:\n",
    "    \"\"\"DMT Nexus is a service to validate your models.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._data = {}\n",
    "        self._analyses = {}\n",
    "        \n",
    "    def __upload_data(self, data, phenomenon=None):\n",
    "        \"\"\"Upload data\"\"\"\n",
    "        if not phenomenon and not hasattr(data, \"phenomenon\"):\n",
    "            raise ValueError(\n",
    "                \"No phenomenon provided for the data to be uploaded.\")\n",
    "        phenomenon=\\\n",
    "            phenomenon if phenomenon\\\n",
    "            else getattr(data, \"phenomenon\", None)\n",
    "        \n",
    "        if phenomenon not in self._data:\n",
    "            self._data[phenomenon] = []\n",
    "        self._data.append(data)\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __upload_analysis(self, analysis, phenomenon=None):\n",
    "        \"\"\"Upload analysis\"\"\"\n",
    "        if not phenomenon and not hasattr(analysis, \"phenomenon\"):\n",
    "            raise ValueError(\n",
    "                \"No phenomenon provided for the analysis to be uploaded.\")\n",
    "        phenomenon=\\\n",
    "            phenomenon if phenomenon\\\n",
    "            else getattr(analysis, \"phenomenon\", None)\n",
    "        \n",
    "        if phenomenon not in self._analysis:\n",
    "            self._analysis[phenomenon] = []\n",
    "        self._analysis.append(analysis)\n",
    "        return len(self._analysis)\n",
    "            \n",
    "    def upload(self, entity_type, entity, phenomenon=None):\n",
    "        \"\"\"Upload an entity\n",
    "            Arguments\n",
    "            ----------\n",
    "            entity_type: EntityType\n",
    "            entity: an instance of entity_type.\n",
    "            \n",
    "            Return\n",
    "            -----------\n",
    "            an identifier to track the uploaded data.\"\"\"\n",
    "        \n",
    "        if entity_type == EntityType.DATA:\n",
    "            return\\\n",
    "                self.__upload_data(\n",
    "                    entity,\n",
    "                    phenomenon=phenomenon)\n",
    "        if entity_type == EntityType.ANALYSIS:\n",
    "            return self.__upload_analysis(\n",
    "                    entity,\n",
    "                    phenomenon=phenomenon)\n",
    "        raise ValueError(\n",
    "            \"Uploading of entity type {} is not yet supported.\".format(entity_type))\n",
    "        \n",
    "        def get_data(self, phenomenon):\n",
    "            \"\"\"Get data by phenomenon\"\"\"\n",
    "            return self._data.get(phenomenon, [])\n",
    "        \n",
    "        def get_analyses(self, phenomenon):\n",
    "            \"\"\"Get analyses by phenomenon\"\"\"\n",
    "            return self._data.get(phenomenon, [])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EntityType.DATA == EntityType.DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDensityValidation(AIBase):\n",
    "    \"\"\"Validate cell densities\"\"\"\n",
    "    def __init__(self):\n",
    "        self._reference_data={\n",
    "            \"data\": pd.DataFrame({\n",
    "                        \"mean\": [0.8, 12., 20., 24., 10., 2.],\n",
    "                        \"std\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]},\n",
    "                        index=pd.Index(\n",
    "                            [\"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\"],\n",
    "                            name=\"layer\")),\n",
    "            \"provenance\": {\n",
    "                \"uri\": \"this notebook\",\n",
    "                \"citation\": {}}}\n",
    "        \n",
    "        \n",
    "    def __call__(self,\n",
    "        model,\n",
    "        *args, **kwargs):\n",
    "        \"\"\"Call Me\"\"\"\n",
    "        model_measurement=\\\n",
    "            self.adapter.get_cell_density(\n",
    "                model, \n",
    "                *args, **kwargs)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5b3b663fd18a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcdv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ac1af95f7ee5>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     14\u001b[0m     def __call__(self,\n\u001b[1;32m     15\u001b[0m         model):\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity.DATA\n"
     ]
    }
   ],
   "source": [
    "print(Entity.DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
